{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Time Series Prediction Using LSTM\n",
    "In this example, we will implement an LSTM using mlpack to forecast multivariate time series data.\n",
    "This notebook contains a step by step walkthrough for time series prediction using LSTMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup\n",
    "Include all libraries required to implement this tutorial. These mainly include files from mlpack, ensmallen and armadillo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <mlpack/core.hpp>\n",
    "#include <mlpack/methods/ann/rnn.hpp>\n",
    "#include <mlpack/methods/ann/layer/layer.hpp>\n",
    "#include <mlpack/core/data/scaler_methods/min_max_scaler.hpp>\n",
    "#include <mlpack/methods/ann/init_rules/he_init.hpp>\n",
    "#include <mlpack/methods/ann/loss_functions/mean_squared_error.hpp>\n",
    "#include <ensmallen.hpp>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some convenient namespaces to simplify the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using namespace mlpack;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "using namespace mlpack::ann;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "using namespace arma;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "using namespace std;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Set Model and Training parameters.\n",
    "Set the training parameters for the model.\n",
    "Parameters include the step size for the optimizer, ratio for the train/test split, batch size for training, the look-back parameter for LSTM, and number of hidden cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Testing data is taken from the dataset in this ratio.\n",
    "const double RATIO = 0.1;\n",
    "\n",
    "// Step size of an optimizer.\n",
    "const double STEP_SIZE = 5e-4;\n",
    "\n",
    "// Number of cells in the LSTM (hidden layers in standard terms).\n",
    "// NOTE: you may play with this variable in order to further optimize the\n",
    "// model (as more cells are added, accuracy is likely to go up, but training\n",
    "// time may take longer).\n",
    "const int H1 = 16;\n",
    "\n",
    "// Number of data points in each iteration of SGD.\n",
    "const size_t BATCH_SIZE = 16;\n",
    "\n",
    "// Nunmber of timesteps to look backward for in the RNN.\n",
    "const int rho = 16;\n",
    "\n",
    "// Max Rho for LSTM.\n",
    "const int maxRho = rho;\n",
    "\n",
    "// Number of epochs for training. Increase the epochs for better results.\n",
    "const int EPOCHS = 15;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set paths for the dataset, trained model and final predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Change the names of these files as necessary. They should be correct\n",
    "// already, if your program's working directory contains the data and/or\n",
    "// model.\n",
    "const std::string dataFile = \"Google2016-2019.csv\";\n",
    "// Path where the model will be saved.\n",
    "const std::string modelFile = \"lstm_multi.bin\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Loading the Dataset.\n",
    "Dataset will be loaded and preprocessed for the model. If we want to predict the Google stock price correctly then we need to consider the volume of the stocks traded, the closing, opening, high and low values of the stock price from the previous days. This is a time series problem. We will create data for the training of the RNN model that will go back 25 business days in the past for each time step. We will convert the input data to the time series format the RNN requires. We will take 30 % of the latest data as our test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            0            0            0            0            0\n",
      "   6.6826e+02   2.6320e+06   6.7100e+02   6.7230e+02   6.6328e+02\n",
      "   6.8004e+02   2.1697e+06   6.7897e+02   6.8033e+02   6.7300e+02\n",
      "   6.8411e+02   1.9314e+06   6.8300e+02   6.8743e+02   6.8141e+02\n"
     ]
    }
   ],
   "source": [
    "arma::mat dataset;\n",
    "\n",
    "// Armadillo is column-major, so each column represents one data point\n",
    "// and each row represents one dimension.\n",
    "mlpack::data::Load(dataFile, dataset, true);\n",
    "// Visualize the first the rows of the dataset.\n",
    "dataset.submat(1, 0, dataset.n_rows - 1, 3).t().print()\n",
    "// Here each row represents: close, volume, open, high, low.\n",
    "// The 0s in the first row represent the column headers.\n",
    "// We have printed the transposed dataset since\n",
    "// Armadillo represents a data in a column-major format,\n",
    "// but we want to print in a row-major format (one data point per row)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Preprocess the dataset.\n",
    "Since columns such as date and time are not needed we will drop them. We will also scale the data to increase stability in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "// The CSV file has a header, so it is necessary to remove it. In Armadillo's\n",
    "// representation it is the first column.\n",
    "// The first column in the CSV is the date which is not required, therefore\n",
    "// we remove it also (first row in in arma::mat).\n",
    "\n",
    "dataset = dataset.submat(1, 1, dataset.n_rows - 1, dataset.n_cols - 1);\n",
    "\n",
    "// We have 5 input data columns and 2 output columns (target).\n",
    "size_t inputSize = 5, outputSize = 2;\n",
    "\n",
    "// Split the dataset into training and validation sets.\n",
    "arma::mat trainData;\n",
    "arma::mat testData;\n",
    "data::Split(dataset, trainData, testData, RATIO, false);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlpack::data::MinMaxScaler scale;\n",
    "// Fit scaler only on training data.\n",
    "scale.Fit(trainData);\n",
    "scale.Transform(trainData, trainData);\n",
    "scale.Transform(testData, testData);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Create Time Series Dataset.\n",
    "The time series data for training the model contains the Closing stock price, the Volume of stocks traded, Opening stock price, Highest and Lowest stock price for 'rho' days in the past.\n",
    "The two target variables (multivariate) we want to predict are the Highest stock price and Lowest stock price (high, low) for the next day! This function essentially reshapes the dataset into desired shape for LSTMs. Here the dataset will be converted to a cube having rho slices where each slice contains values of input features for that day. Similarly the prediction data i.e. high and low of stock market for corresponding input features are stacked behind one another for rho days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "template<typename InputDataType = arma::mat,\n",
    "         typename DataType = arma::cube,\n",
    "         typename LabelType = arma::cube>\n",
    "void CreateTimeSeriesData(InputDataType dataset,\n",
    "                          DataType& X,\n",
    "                          LabelType& y,\n",
    "                          const size_t rho,\n",
    "                          const size_t inputSize = 5,\n",
    "                          const size_t outputSize = 2)\n",
    "{\n",
    "  X.set_size(inputSize, dataset.n_cols - rho + 1, rho);\n",
    "  y.set_size(outputSize, dataset.n_cols - rho + 1, rho);\n",
    "  for (size_t i = 0; i < dataset.n_cols - rho; i++)\n",
    "  {\n",
    "    X.subcube(arma::span(), arma::span(i), arma::span()) =\n",
    "        dataset.submat(arma::span(), arma::span(i, i + rho - 1));\n",
    "    y.subcube(arma::span(), arma::span(i), arma::span()) =\n",
    "        dataset.submat(arma::span(3, 4), arma::span(i + 1, i + rho));\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "// We need to individually create training and testing time series data.\n",
    "arma::cube trainX, trainY, testX, testY;\n",
    "// Create training sets for one-step-ahead regression.\n",
    "CreateTimeSeriesData(trainData, trainX, trainY, rho);\n",
    "// Create test sets for one-step-ahead regression.\n",
    "CreateTimeSeriesData(testData, testX, testY, rho);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Create the Model\n",
    "We add 3 LSTM modules that will be stacked one after the other in the RNN, implementing an efficient stacked RNN. Finally, the output will have 2 units the (high, low) values of the stock price for the next day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlpack::ann::RNN<mlpack::ann::MeanSquaredError<>, mlpack::ann::HeInitialization> model(rho);\n",
    "// Create the model architecture.\n",
    "model.Add<mlpack::ann::IdentityLayer<> >();\n",
    "model.Add<mlpack::ann::LSTM<> >(inputSize, H1, maxRho);\n",
    "model.Add<mlpack::ann::Dropout<> >(0.6);\n",
    "model.Add<mlpack::ann::LeakyReLU<> >();\n",
    "model.Add<mlpack::ann::LSTM<> >(H1, H1, maxRho);\n",
    "model.Add<mlpack::ann::Dropout<> >(0.6);\n",
    "model.Add<mlpack::ann::LeakyReLU<> >();\n",
    "model.Add<mlpack::ann::LSTM<> >(H1, H1, maxRho);\n",
    "model.Add<mlpack::ann::ReLULayer<> >();\n",
    "model.Add<mlpack::ann::Linear<> >(H1, outputSize);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Training the model.\n",
    "We will use ensmallen to create the optimizer and train the model. For more details refer to the [documentation](https://www.ensmallen.org/docs.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ens::Adam optimizer(\n",
    "    STEP_SIZE, // Step size of the optimizer.\n",
    "    BATCH_SIZE, // Batch size. Number of data points that are used in each iteration.\n",
    "    0.9, // Exponential decay rate for the first moment estimates.\n",
    "    0.999, // Exponential decay rate for the weighted infinity norm estimates.\n",
    "    1e-8, // Value used to initialise the mean squared gradient parameter.\n",
    "    trainData.n_cols * EPOCHS, // Max number of iterations.\n",
    "    1e-8,// Tolerance.\n",
    "    true);\n",
    "\n",
    "optimizer.Tolerance() = -1;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will use ensmallen callbacks to train the model. We will be using Adam optimizer. We will terminate optimization when the loss stops decreasing or doesn't show any improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/244\n",
      "0.502446\n",
      "42/42 [==================================================] 100% - 0s 9ms/step - loss: 0.502446\n",
      "Epoch 2/244\n",
      "0.430152\n",
      "42/42 [==================================================] 100% - 0s 10ms/step - loss: 0.430152\n",
      "Epoch 3/244\n",
      "0.207513\n",
      "42/42 [==================================================] 100% - 0s 10ms/step - loss: 0.207513\n",
      "Epoch 4/244\n",
      "0.178334\n",
      "42/42 [==================================================] 100% - 0s 10ms/step - loss: 0.178334\n",
      "Epoch 5/244\n",
      "0.162722\n",
      "42/42 [==================================================] 100% - 0s 10ms/step - loss: 0.162722\n",
      "Epoch 6/244\n",
      "0.145162\n",
      "42/42 [==================================================] 100% - 0s 10ms/step - loss: 0.145162\n",
      "Epoch 7/244\n",
      "0.127502\n",
      "42/42 [==================================================] 100% - 0s 9ms/step - loss: 0.127502\n",
      "Epoch 8/244\n",
      "0.11174\n",
      "42/42 [==================================================] 100% - 0s 10ms/step - loss: 0.11174\n",
      "Epoch 9/244\n",
      "0.0978461\n",
      "42/42 [==================================================] 100% - 0s 10ms/step - loss: 0.0978461\n",
      "Epoch 10/244\n",
      "0.0868993\n",
      "42/42 [==================================================] 100% - 0s 10ms/step - loss: 0.0868993\n",
      "Epoch 11/244\n",
      "0.0761016\n",
      "42/42 [==================================================] 100% - 0s 10ms/step - loss: 0.0761016\n",
      "Epoch 12/244\n",
      "0.0704167\n",
      "42/42 [==================================================] 100% - 0s 10ms/step - loss: 0.0704167\n",
      "Epoch 13/244\n",
      "0.0641131\n",
      "42/42 [==================================================] 100% - 0s 10ms/step - loss: 0.0641131\n",
      "Epoch 14/244\n",
      "0.0592488\n",
      "42/42 [==================================================] 100% - 0s 11ms/step - loss: 0.0592488\n",
      "Epoch 15/244\n",
      "0.0556059\n",
      "42/42 [==================================================] 100% - 0s 10ms/step - loss: 0.0556059\n",
      "Epoch 16/244\n"
     ]
    }
   ],
   "source": [
    "// Train the model.\n",
    "model.Train(trainX,\n",
    "            trainY,\n",
    "            optimizer,\n",
    "            // PrintLoss Callback prints loss for each epoch.\n",
    "            ens::PrintLoss(),\n",
    "            // Progressbar Callback prints progress bar for each epoch.\n",
    "            // Here 40 signifies width of progress bar.\n",
    "            ens::ProgressBar(40),\n",
    "            // Stops the optimization process if the loss stops decreasing\n",
    "            // or no improvement has been made. This will terminate the\n",
    "            // optimization once we obtain a minima on training set.\n",
    "            ens::EarlyStopAtMinLoss());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Running inference and printing the results\n",
    "Since the predicted is in form of a cube we need to convert it to matrix with predicted values. Then we take the inverse transform to convert it to meaningful data. The prediction results are the (high, low) for the next day and come from the last slice of the prediction. The last 2 columns are the predictions, the preceding columns are the data used to generate those predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Run Inference.\n",
    "arma::cube predOutP;\n",
    "model.Predict(testX, predOutP);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted Google stock (high, low) for the last day is: \n",
      " (1108.29, 1029.65)\n"
     ]
    }
   ],
   "source": [
    "arma::mat flatDataAndPreds = testX.slice(testX.n_slices - 1);\n",
    "// The prediction results are the (high, low) for the next day and come from\n",
    "// the last slice from the prediction.\n",
    "flatDataAndPreds.rows(flatDataAndPreds.n_rows - 2,\n",
    "    flatDataAndPreds.n_rows - 1) = predOutP.slice(predOutP.n_slices - 1);\n",
    "\n",
    "scale.InverseTransform(flatDataAndPreds, flatDataAndPreds);\n",
    "// We need to remove the last column because it was not used for training\n",
    "// (there is no next day to predict).\n",
    "flatDataAndPreds.shed_col(flatDataAndPreds.n_cols - 1);\n",
    "\n",
    "// NOTE: we do not have the last data point in the input for the prediction\n",
    "// because we did not use it for the training, therefore the prediction result\n",
    "// will be for the day before. In your own application you may of course load\n",
    "// any dataset for prediction.\n",
    "std::cout << \"The predicted Google stock (high, low) for the last day is: \" << std::endl\n",
    "    << \" (\" << flatDataAndPreds(flatDataAndPreds.n_rows - 2, flatDataAndPreds.n_cols - 1) << \", \"\n",
    "    << flatDataAndPreds(flatDataAndPreds.n_rows - 1, flatDataAndPreds.n_cols - 1) << \")\"\n",
    "    << std::endl;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Loading and Saving Models\n",
    "In the real world, we won't be training the model from scratch everytime we need to run inference.\n",
    "We will save the model once and load it as many times as we want for either training or inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Save the model.\n",
    "mlpack::data::Save(modelFile, \"LSTMMulti\", model);\n",
    "// Load the model.\n",
    "RNN<mlpack::ann::MeanSquaredError<>, mlpack::ann::HeInitialization> trainedModel(rho);\n",
    "mlpack::data::Load(modelFile, \"LSTMMulti\", trainedModel);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "C++11",
   "language": "C++11",
   "name": "xcpp11"
  },
  "language_info": {
   "codemirror_mode": "text/x-c++src",
   "file_extension": ".cpp",
   "mimetype": "text/x-c++src",
   "name": "c++",
   "version": "11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://lab.mlpack.org/v2/gh/mlpack/examples/master?urlpath=lab%2Ftree%2Freinforcement_learning_gym%2Fmountain_car_dqn%2Fmountain_car_dqn.ipynb)\n",
    "\n",
    "You can easily run this notebook at https://lab.mlpack.org/\n",
    "\n",
    "In this notebook, we show how to use a simple DQN to train an agent to solve the [MountainCar](https://gym.openai.com/envs/MountainCar-v0) environment. \n",
    "\n",
    "We make the agent train and test on OpenAI Gym toolkit's GUI interface provided through a distributed infrastructure (TCP API). More details can be found [here](https://github.com/zoq/gym_tcp_api).\n",
    "\n",
    "A video of the trained agent can be seen in the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Including necessary libraries and namespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <mlpack/xeus-cling.hpp>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <mlpack/core.hpp>\n",
    "#include <mlpack/methods/ann/ffn.hpp>\n",
    "#include <mlpack/methods/reinforcement_learning/q_learning.hpp>\n",
    "#include <mlpack/methods/reinforcement_learning/q_networks/simple_dqn.hpp>\n",
    "#include <mlpack/methods/reinforcement_learning/environment/env_type.hpp>\n",
    "#include <mlpack/methods/reinforcement_learning/policy/greedy_policy.hpp>\n",
    "#include <mlpack/methods/reinforcement_learning/training_config.hpp>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Used to run the agent on gym's environment (provided externally) for testing.\n",
    "#include <gym/environment.hpp>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Used to generate and display a video of the trained agent.\n",
    "#include \"xwidgets/ximage.hpp\"\n",
    "#include \"xwidgets/xvideo.hpp\"\n",
    "#include \"xwidgets/xaudio.hpp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "using namespace mlpack;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "using namespace mlpack::ann;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "using namespace ens;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "using namespace mlpack::rl;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set up the state and action space.\n",
    "DiscreteActionEnv::State::dimension = 2;\n",
    "DiscreteActionEnv::Action::size = 3;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set up the network.\n",
    "FFN<MeanSquaredError<>, GaussianInitialization> network(\n",
    "    MeanSquaredError<>(), GaussianInitialization(0, 1));\n",
    "network.Add<Linear<>>(DiscreteActionEnv::State::dimension, 128);\n",
    "network.Add<ReLULayer<>>();\n",
    "network.Add<Linear<>>(128, DiscreteActionEnv::Action::size);\n",
    "// Set up the network.\n",
    "SimpleDQN<> model(network);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set up the policy method.\n",
    "GreedyPolicy<DiscreteActionEnv> policy(1.0, 1000, 0.1, 0.99);\n",
    "RandomReplay<DiscreteActionEnv> replayMethod(32, 10000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set up training configurations.\n",
    "TrainingConfig config;\n",
    "config.TargetNetworkSyncInterval() = 100;\n",
    "config.ExplorationSteps() = 400;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set up DQN agent.\n",
    "QLearning<DiscreteActionEnv, decltype(model), AdamUpdate, decltype(policy), decltype(replayMethod)>\n",
    "    agent(config, model, policy, replayMethod);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation for training the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set up the gym training environment.\n",
    "gym::Environment env(\"gym.kurg.org\", \"4040\", \"MountainCar-v0\");\n",
    "\n",
    "// Initializing training variables.\n",
    "std::vector<double> returnList;\n",
    "size_t episodes = 0;\n",
    "bool converged = true;\n",
    "\n",
    "// The number of episode returns to keep track of.\n",
    "size_t consecutiveEpisodes = 50;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important point to note for Mountain Car setup is that for each step that the car does not reach the goal located at position `0.5`, the environment returns a reward of `-1`. Now, since the agentâ€™s reward never changes until completion of the episode, it is difficult for our algorithm to improve until it randomly reaches the top of the hill.\n",
    "\n",
    "That is unless we modify the reward by giving an additional `0.5` reward for every time the agent managed to drag the car in the backward direction (i.e position < `-0.8`). This was important to gain momentum to climb the hill.\n",
    "\n",
    "This minor tweak can greatly increase sample efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Function to train the agent on the MountainCar-v0 gym environment.\n",
    "void Train(const size_t numSteps)\n",
    "{\n",
    "  agent.Deterministic() = false;\n",
    "  std::cout << \"Training for \" << numSteps << \" steps.\" << std::endl;\n",
    "  while (agent.TotalSteps() < numSteps)\n",
    "  {\n",
    "    double episodeReturn = 0;\n",
    "    double adjustedEpisodeReturn = 0;\n",
    "    env.reset();\n",
    "    do\n",
    "    {\n",
    "      agent.State().Data() = env.observation;\n",
    "      agent.SelectAction();\n",
    "      arma::mat action = {double(agent.Action().action)};\n",
    "\n",
    "      env.step(action);\n",
    "      DiscreteActionEnv::State nextState;\n",
    "      nextState.Data() = env.observation;\n",
    "      \n",
    "      // Use an adjusted reward for task completion.\n",
    "      double adjustedReward = env.reward;\n",
    "      if (nextState.Data()[0] < -0.8)\n",
    "        adjustedReward += 0.5;\n",
    "\n",
    "      replayMethod.Store(agent.State(), agent.Action(), adjustedReward, nextState,\n",
    "          env.done, 0.99);\n",
    "      episodeReturn += env.reward;\n",
    "      adjustedEpisodeReturn += adjustedReward;\n",
    "      agent.TotalSteps()++;\n",
    "      if (agent.Deterministic() || agent.TotalSteps() < config.ExplorationSteps())\n",
    "        continue;\n",
    "      agent.TrainAgent();\n",
    "    } while (!env.done);\n",
    "    returnList.push_back(episodeReturn);\n",
    "    episodes += 1;\n",
    "\n",
    "    if (returnList.size() > consecutiveEpisodes)\n",
    "      returnList.erase(returnList.begin());\n",
    "        \n",
    "    double averageReturn = std::accumulate(returnList.begin(),\n",
    "                                           returnList.end(), 0.0) /\n",
    "                           returnList.size();\n",
    "    if(episodes % 5 == 0)\n",
    "    {\n",
    "      std::cout << \"Avg return in last \" << consecutiveEpisodes\n",
    "          << \" episodes: \" << averageReturn\n",
    "          << \"\\t Episode return: \" << episodeReturn\n",
    "          << \"\\t Adjusted return: \" << adjustedEpisodeReturn\n",
    "          << \"\\t Total steps: \" << agent.TotalSteps() << std::endl;\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note here that `Episode return:` is the actual (environment's) return, whereas `Adjusted return:` is the return calculated from the adjusted reward function we described earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let the training begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 15000 steps.\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 1000\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 2000\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 3000\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -190.5\t Total steps: 4000\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -200\t Total steps: 5000\n",
      "Avg return in last 50 episodes: -200\t Episode return: -200\t Adjusted return: -181.5\t Total steps: 6000\n",
      "Avg return in last 50 episodes: -198.486\t Episode return: -200\t Adjusted return: -190.5\t Total steps: 6947\n",
      "Avg return in last 50 episodes: -198.2\t Episode return: -200\t Adjusted return: -200\t Total steps: 7928\n",
      "Avg return in last 50 episodes: -198.4\t Episode return: -200\t Adjusted return: -200\t Total steps: 8928\n",
      "Avg return in last 50 episodes: -197.3\t Episode return: -149\t Adjusted return: -133.5\t Total steps: 9865\n",
      "Avg return in last 50 episodes: -195.1\t Episode return: -157\t Adjusted return: -143\t Total steps: 10755\n",
      "Avg return in last 50 episodes: -194.12\t Episode return: -194\t Adjusted return: -177.5\t Total steps: 11706\n",
      "Avg return in last 50 episodes: -192.5\t Episode return: -200\t Adjusted return: -169\t Total steps: 12625\n",
      "Avg return in last 50 episodes: -190.48\t Episode return: -200\t Adjusted return: -171\t Total steps: 13524\n",
      "Avg return in last 50 episodes: -188.62\t Episode return: -200\t Adjusted return: -174\t Total steps: 14431\n"
     ]
    }
   ],
   "source": [
    "// Training the agent for a total of at least 75 episodes.\n",
    "Train(200*75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Total steps: 162\t Total reward: -162\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5e5df1e20d84017b0b9ebee90bf6728",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter widget"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.Deterministic() = true;\n",
    "\n",
    "// Creating and setting up the gym environment for testing.\n",
    "gym::Environment envTest(\"gym.kurg.org\", \"4040\", \"MountainCar-v0\");\n",
    "envTest.monitor.start(\"./dummy/\", true, true);\n",
    "\n",
    "// Resets the environment.\n",
    "envTest.reset();\n",
    "envTest.render();\n",
    "\n",
    "double totalReward = 0;\n",
    "size_t totalSteps = 0;\n",
    "\n",
    "// Testing the agent on gym's environment.\n",
    "while (1)\n",
    "{\n",
    "  // State from the environment is passed to the agent's internal representation.\n",
    "  agent.State().Data() = envTest.observation;\n",
    "\n",
    "  // With the given state, the agent selects an action according to its defined policy.\n",
    "  agent.SelectAction();\n",
    "\n",
    "  // Action to take, decided by the policy.\n",
    "  arma::mat action = {double(agent.Action().action)};\n",
    "\n",
    "  envTest.step(action);\n",
    "  totalReward += envTest.reward;\n",
    "  totalSteps += 1;\n",
    "\n",
    "  if (envTest.done)\n",
    "  {\n",
    "    std::cout << \" Total steps: \" << totalSteps << \"\\t Total reward: \"\n",
    "        << totalReward << std::endl;\n",
    "    break;\n",
    "  }\n",
    "\n",
    "  // Uncomment the following lines to see the reward and action in each step.\n",
    "  // std::cout << \" Current step: \" << totalSteps << \"\\t current reward: \"\n",
    "  //   << totalReward << \"\\t Action taken: \" << action;\n",
    "}\n",
    "\n",
    "envTest.close();\n",
    "std::string url = envTest.url();\n",
    "\n",
    "auto video = xw::video_from_url(url).finalize();\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A little more training..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 60000 steps.\n",
      "Avg return in last 50 episodes: -188.62\t Episode return: -200\t Adjusted return: -184.5\t Total steps: 15431\n",
      "Avg return in last 50 episodes: -186.08\t Episode return: -142\t Adjusted return: -127.5\t Total steps: 16251\n",
      "Avg return in last 50 episodes: -183.98\t Episode return: -159\t Adjusted return: -144.5\t Total steps: 17127\n",
      "Avg return in last 50 episodes: -182.22\t Episode return: -173\t Adjusted return: -158.5\t Total steps: 18039\n",
      "Avg return in last 50 episodes: -180.28\t Episode return: -158\t Adjusted return: -142.5\t Total steps: 18879\n",
      "Avg return in last 50 episodes: -178.6\t Episode return: -162\t Adjusted return: -148\t Total steps: 19685\n",
      "Avg return in last 50 episodes: -177.32\t Episode return: -196\t Adjusted return: -177.5\t Total steps: 20572\n",
      "Avg return in last 50 episodes: -173.96\t Episode return: -157\t Adjusted return: -142\t Total steps: 21323\n",
      "Avg return in last 50 episodes: -172.9\t Episode return: -177\t Adjusted return: -161.5\t Total steps: 22169\n",
      "Avg return in last 50 episodes: -172.44\t Episode return: -200\t Adjusted return: -183.5\t Total steps: 23053\n",
      "Avg return in last 50 episodes: -168.92\t Episode return: -147\t Adjusted return: -132.5\t Total steps: 23877\n",
      "Avg return in last 50 episodes: -169.54\t Episode return: -156\t Adjusted return: -141.5\t Total steps: 24728\n",
      "Avg return in last 50 episodes: -168.72\t Episode return: -126\t Adjusted return: -113.5\t Total steps: 25563\n",
      "Avg return in last 50 episodes: -166.14\t Episode return: -150\t Adjusted return: -132\t Total steps: 26346\n",
      "Avg return in last 50 episodes: -164.4\t Episode return: -159\t Adjusted return: -145\t Total steps: 27099\n",
      "Avg return in last 50 episodes: -163.24\t Episode return: -159\t Adjusted return: -146.5\t Total steps: 27847\n",
      "Avg return in last 50 episodes: -161.32\t Episode return: -162\t Adjusted return: -138.5\t Total steps: 28638\n",
      "Avg return in last 50 episodes: -162.28\t Episode return: -156\t Adjusted return: -143.5\t Total steps: 29437\n",
      "Avg return in last 50 episodes: -161.14\t Episode return: -160\t Adjusted return: -139.5\t Total steps: 30226\n",
      "Avg return in last 50 episodes: -159.08\t Episode return: -150\t Adjusted return: -138\t Total steps: 31007\n",
      "Avg return in last 50 episodes: -157.86\t Episode return: -200\t Adjusted return: -182\t Total steps: 31770\n",
      "Avg return in last 50 episodes: -156.42\t Episode return: -198\t Adjusted return: -178\t Total steps: 32549\n",
      "Avg return in last 50 episodes: -154.26\t Episode return: -153\t Adjusted return: -133\t Total steps: 33276\n",
      "Avg return in last 50 episodes: -154.5\t Episode return: -148\t Adjusted return: -134.5\t Total steps: 34071\n",
      "Avg return in last 50 episodes: -155.74\t Episode return: -166\t Adjusted return: -148.5\t Total steps: 34886\n",
      "Avg return in last 50 episodes: -156.8\t Episode return: -163\t Adjusted return: -141.5\t Total steps: 35687\n",
      "Avg return in last 50 episodes: -156.86\t Episode return: -169\t Adjusted return: -154.5\t Total steps: 36481\n",
      "Avg return in last 50 episodes: -157.14\t Episode return: -152\t Adjusted return: -132\t Total steps: 37294\n",
      "Avg return in last 50 episodes: -158\t Episode return: -200\t Adjusted return: -185.5\t Total steps: 38126\n",
      "Avg return in last 50 episodes: -157.86\t Episode return: -155\t Adjusted return: -139\t Total steps: 38900\n",
      "Avg return in last 50 episodes: -155.84\t Episode return: -115\t Adjusted return: -98\t Total steps: 39562\n",
      "Avg return in last 50 episodes: -156.78\t Episode return: -198\t Adjusted return: -172.5\t Total steps: 40388\n",
      "Avg return in last 50 episodes: -157.08\t Episode return: -157\t Adjusted return: -136.5\t Total steps: 41130\n",
      "Avg return in last 50 episodes: -157.14\t Episode return: -165\t Adjusted return: -142\t Total steps: 41928\n",
      "Avg return in last 50 episodes: -154.6\t Episode return: -189\t Adjusted return: -176\t Total steps: 42616\n",
      "Avg return in last 50 episodes: -153.48\t Episode return: -140\t Adjusted return: -124.5\t Total steps: 43361\n",
      "Avg return in last 50 episodes: -151.84\t Episode return: -152\t Adjusted return: -137\t Total steps: 44073\n",
      "Avg return in last 50 episodes: -148.26\t Episode return: -122\t Adjusted return: -105.5\t Total steps: 44707\n",
      "Avg return in last 50 episodes: -146.96\t Episode return: -195\t Adjusted return: -173.5\t Total steps: 45474\n",
      "Avg return in last 50 episodes: -145.04\t Episode return: -125\t Adjusted return: -110\t Total steps: 46152\n",
      "Avg return in last 50 episodes: -146.58\t Episode return: -163\t Adjusted return: -139.5\t Total steps: 46891\n",
      "Avg return in last 50 episodes: -144.24\t Episode return: -150\t Adjusted return: -137.5\t Total steps: 47600\n",
      "Avg return in last 50 episodes: -143.48\t Episode return: -148\t Adjusted return: -135.5\t Total steps: 48304\n",
      "Avg return in last 50 episodes: -143.14\t Episode return: -157\t Adjusted return: -141.5\t Total steps: 49085\n",
      "Avg return in last 50 episodes: -143.2\t Episode return: -150\t Adjusted return: -137.5\t Total steps: 49776\n",
      "Avg return in last 50 episodes: -143.96\t Episode return: -158\t Adjusted return: -139.5\t Total steps: 50559\n",
      "Avg return in last 50 episodes: -143.8\t Episode return: -132\t Adjusted return: -116\t Total steps: 51263\n",
      "Avg return in last 50 episodes: -147.06\t Episode return: -166\t Adjusted return: -145.5\t Total steps: 52060\n",
      "Avg return in last 50 episodes: -146.9\t Episode return: -154\t Adjusted return: -139\t Total steps: 52819\n",
      "Avg return in last 50 episodes: -147.96\t Episode return: -114\t Adjusted return: -97.5\t Total steps: 53550\n",
      "Avg return in last 50 episodes: -147.22\t Episode return: -155\t Adjusted return: -136.5\t Total steps: 54252\n",
      "Avg return in last 50 episodes: -147.6\t Episode return: -115\t Adjusted return: -100\t Total steps: 54980\n",
      "Avg return in last 50 episodes: -148.16\t Episode return: -145\t Adjusted return: -129\t Total steps: 55712\n",
      "Avg return in last 50 episodes: -146\t Episode return: -163\t Adjusted return: -144.5\t Total steps: 56385\n",
      "Avg return in last 50 episodes: -147.92\t Episode return: -200\t Adjusted return: -178.5\t Total steps: 57172\n",
      "Avg return in last 50 episodes: -146.86\t Episode return: -170\t Adjusted return: -150.5\t Total steps: 57902\n",
      "Avg return in last 50 episodes: -146.64\t Episode return: -152\t Adjusted return: -135\t Total steps: 58595\n",
      "Avg return in last 50 episodes: -145.06\t Episode return: -150\t Adjusted return: -128.5\t Total steps: 59313\n",
      "Avg return in last 50 episodes: -144.84\t Episode return: -161\t Adjusted return: -135\t Total steps: 60061\n"
     ]
    }
   ],
   "source": [
    "// Training the same agent for a total of at least 300 episodes.\n",
    "Train(200*300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final agent testing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Total steps: 104\t Total reward: -104\n",
      "https://gym.kurg.org/b6827842a41c4/output.webm"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88ed7fba2dde4db59c6261417e909c48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter widget"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.Deterministic() = true;\n",
    "\n",
    "// Creating and setting up the gym environment for testing.\n",
    "gym::Environment envTest(\"gym.kurg.org\", \"4040\", \"MountainCar-v0\");\n",
    "envTest.monitor.start(\"./dummy/\", true, true);\n",
    "\n",
    "// Resets the environment.\n",
    "envTest.reset();\n",
    "envTest.render();\n",
    "\n",
    "double totalReward = 0;\n",
    "size_t totalSteps = 0;\n",
    "\n",
    "// Testing the agent on gym's environment.\n",
    "while (1)\n",
    "{\n",
    "  // State from the environment is passed to the agent's internal representation.\n",
    "  agent.State().Data() = envTest.observation;\n",
    "\n",
    "  // With the given state, the agent selects an action according to its defined policy.\n",
    "  agent.SelectAction();\n",
    "\n",
    "  // Action to take, decided by the policy.\n",
    "  arma::mat action = {double(agent.Action().action)};\n",
    "\n",
    "  envTest.step(action);\n",
    "  totalReward += envTest.reward;\n",
    "  totalSteps += 1;\n",
    "\n",
    "  if (envTest.done)\n",
    "  {\n",
    "    std::cout << \" Total steps: \" << totalSteps << \"\\t Total reward: \"\n",
    "        << totalReward << std::endl;\n",
    "    break;\n",
    "  }\n",
    "\n",
    "  // Uncomment the following lines to see the reward and action in each step.\n",
    "  // std::cout << \" Current step: \" << totalSteps << \"\\t current reward: \"\n",
    "  //   << totalReward << \"\\t Action taken: \" << action;\n",
    "}\n",
    "\n",
    "envTest.close();\n",
    "std::string url = envTest.url();\n",
    "std::cout << url;\n",
    "auto video = xw::video_from_url(url).finalize();\n",
    "video"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "C++14",
   "language": "C++14",
   "name": "xcpp14"
  },
  "language_info": {
   "codemirror_mode": "text/x-c++src",
   "file_extension": ".cpp",
   "mimetype": "text/x-c++src",
   "name": "c++",
   "version": "14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

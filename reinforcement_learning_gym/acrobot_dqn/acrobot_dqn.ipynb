{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://lab.mlpack.org/v2/gh/mlpack/examples/master?urlpath=lab%2Ftree%2Freinforcement_learning_gym%2Facrobot_dqn%2Facrobot_dqn.ipynb)\n",
    "\n",
    "You can easily run this notebook at https://lab.mlpack.org/\n",
    "\n",
    "This notebook is shows how to get use 3-Step Double DQN with Prioritized Replay to train an agent to get high scores for the [Acrobot](https://gym.openai.com/envs/Acrobot-v1) environment. \n",
    "\n",
    "We make the agent train and test on OpenAI Gym toolkit's GUI interface provided through a distributed infrastructure (TCP API). More details can be found [here](https://github.com/zoq/gym_tcp_api).\n",
    "\n",
    "A video of the trained agent can be seen in the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Including necessary libraries and namespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <mlpack/xeus-cling.hpp>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <mlpack/methods/ann/core.hpp>\n",
    "#include <mlpack/methods/ann/ffn.hpp>\n",
    "#include <mlpack/methods/reinforcement_learning/q_learning.hpp>\n",
    "#include <mlpack/methods/reinforcement_learning/q_networks/simple_dqn.hpp>\n",
    "#include <mlpack/methods/reinforcement_learning/environment/env_type.hpp>\n",
    "#include <mlpack/methods/reinforcement_learning/policy/greedy_policy.hpp>\n",
    "#include <mlpack/methods/reinforcement_learning/training_config.hpp>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Used to run the agent on gym's environment (provided externally) for testing.\n",
    "#include <gym/environment.hpp>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Used to generate and display a video of the trained agent.\n",
    "#include \"xwidgets/ximage.hpp\"\n",
    "#include \"xwidgets/xvideo.hpp\"\n",
    "#include \"xwidgets/xaudio.hpp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "using namespace mlpack;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "using namespace mlpack::ann;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "using namespace ens;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "using namespace mlpack::rl;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set up the state and action space.\n",
    "DiscreteActionEnv::State::dimension = 6;\n",
    "DiscreteActionEnv::Action::size = 3;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set up the network.\n",
    "FFN<MeanSquaredError<>, RandomInitialization> module(MeanSquaredError<>(), RandomInitialization(-1, 1));\n",
    "module.Add<Linear<>>(DiscreteActionEnv::State::dimension, 64);\n",
    "module.Add<ReLULayer<>>();\n",
    "module.Add<Linear<>>(64, DiscreteActionEnv::Action::size);\n",
    "SimpleDQN<FFN<MeanSquaredError<>, RandomInitialization>> model(module);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set up the policy method.\n",
    "GreedyPolicy<DiscreteActionEnv> policy(1.0, 1000, 0.1, 0.99);\n",
    "// To enable 3-step learning, we set the last parameter of the replay method as 3.\n",
    "PrioritizedReplay<DiscreteActionEnv> replayMethod(64, 5000, 0.6, 3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set up training configurations.\n",
    "TrainingConfig config;\n",
    "config.TargetNetworkSyncInterval() = 100;\n",
    "config.ExplorationSteps() = 500;\n",
    "\n",
    "// We use double Q learning for this example.\n",
    "config.DoubleQLearning() = true;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set up DQN agent.\n",
    "QLearning<DiscreteActionEnv, decltype(model), AdamUpdate, decltype(policy), decltype(replayMethod)>\n",
    "    agent(config, model, policy, replayMethod);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation for training the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Set up the gym training environment.\n",
    "gym::Environment env(\"gym.kurg.org\", \"4040\", \"Acrobot-v1\");\n",
    "\n",
    "// Initializing training variables.\n",
    "std::vector<double> returnList;\n",
    "size_t episodes = 0;\n",
    "bool converged = true;\n",
    "\n",
    "// The number of episode returns to keep track of.\n",
    "size_t consecutiveEpisodes = 50;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Function to train the agent on the Acrobot-v1 gym environment.\n",
    "void Train(const size_t numSteps)\n",
    "{\n",
    "  agent.Deterministic() = false;\n",
    "  std::cout << \"Training for \" << numSteps << \" steps.\" << std::endl;\n",
    "  while (agent.TotalSteps() < numSteps)\n",
    "  {\n",
    "    double episodeReturn = 0;\n",
    "    env.reset();\n",
    "    do\n",
    "    {\n",
    "      agent.State().Data() = env.observation;\n",
    "      agent.SelectAction();\n",
    "      arma::mat action = {double(agent.Action().action)};\n",
    "\n",
    "      env.step(action);\n",
    "      DiscreteActionEnv::State nextState;\n",
    "      nextState.Data() = env.observation;\n",
    "\n",
    "      replayMethod.Store(agent.State(), agent.Action(), env.reward, nextState,\n",
    "          env.done, 0.99);\n",
    "      episodeReturn += env.reward;\n",
    "      agent.TotalSteps()++;\n",
    "      if (agent.Deterministic() || agent.TotalSteps() < config.ExplorationSteps())\n",
    "        continue;\n",
    "      agent.TrainAgent();\n",
    "    } while (!env.done);\n",
    "    returnList.push_back(episodeReturn);\n",
    "    episodes += 1;\n",
    "\n",
    "    if (returnList.size() > consecutiveEpisodes)\n",
    "      returnList.erase(returnList.begin());\n",
    "        \n",
    "    double averageReturn = std::accumulate(returnList.begin(),\n",
    "                                           returnList.end(), 0.0) /\n",
    "                           returnList.size();\n",
    "    if(episodes % 5 == 0)\n",
    "    {\n",
    "      std::cout << \"Avg return in last \" << consecutiveEpisodes\n",
    "          << \" episodes: \" << averageReturn\n",
    "          << \"\\t Episode return: \" << episodeReturn\n",
    "          << \"\\t Total steps: \" << agent.TotalSteps() << std::endl;\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let the training begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 25000 steps.\n",
      "Avg return in last 50 episodes: -476.4\t Episode return: -465\t Total steps: 2384\n",
      "Avg return in last 50 episodes: -352.7\t Episode return: -141\t Total steps: 3534\n",
      "Avg return in last 50 episodes: -289.267\t Episode return: -209\t Total steps: 4351\n",
      "Avg return in last 50 episodes: -254.25\t Episode return: -106\t Total steps: 5102\n",
      "Avg return in last 50 episodes: -225.68\t Episode return: -123\t Total steps: 5664\n",
      "Avg return in last 50 episodes: -206.167\t Episode return: -87\t Total steps: 6212\n",
      "Avg return in last 50 episodes: -192.743\t Episode return: -108\t Total steps: 6778\n",
      "Avg return in last 50 episodes: -185.325\t Episode return: -146\t Total steps: 7450\n",
      "Avg return in last 50 episodes: -194.822\t Episode return: -191\t Total steps: 8808\n",
      "Avg return in last 50 episodes: -193.8\t Episode return: -164\t Total steps: 9736\n",
      "Avg return in last 50 episodes: -158.52\t Episode return: -126\t Total steps: 10359\n",
      "Avg return in last 50 episodes: -152.08\t Episode return: -165\t Total steps: 11187\n",
      "Avg return in last 50 episodes: -147.74\t Episode return: -110\t Total steps: 11787\n",
      "Avg return in last 50 episodes: -146.8\t Episode return: -152\t Total steps: 12491\n",
      "Avg return in last 50 episodes: -152.56\t Episode return: -118\t Total steps: 13341\n",
      "Avg return in last 50 episodes: -154.04\t Episode return: -90\t Total steps: 13963\n",
      "Avg return in last 50 episodes: -155.62\t Episode return: -137\t Total steps: 14608\n",
      "Avg return in last 50 episodes: -155.16\t Episode return: -94\t Total steps: 15257\n",
      "Avg return in last 50 episodes: -140.96\t Episode return: -114\t Total steps: 15906\n",
      "Avg return in last 50 episodes: -137.1\t Episode return: -101\t Total steps: 16641\n",
      "Avg return in last 50 episodes: -135.5\t Episode return: -97\t Total steps: 17184\n",
      "Avg return in last 50 episodes: -130.54\t Episode return: -117\t Total steps: 17764\n",
      "Avg return in last 50 episodes: -132.14\t Episode return: -147\t Total steps: 18444\n",
      "Avg return in last 50 episodes: -130.82\t Episode return: -120\t Total steps: 19082\n",
      "Avg return in last 50 episodes: -127.92\t Episode return: -138\t Total steps: 19787\n",
      "Avg return in last 50 episodes: -127.08\t Episode return: -105\t Total steps: 20367\n",
      "Avg return in last 50 episodes: -126.92\t Episode return: -149\t Total steps: 21004\n",
      "Avg return in last 50 episodes: -124.34\t Episode return: -113\t Total steps: 21524\n",
      "Avg return in last 50 episodes: -122.36\t Episode return: -96\t Total steps: 22074\n",
      "Avg return in last 50 episodes: -120.3\t Episode return: -85\t Total steps: 22706\n",
      "Avg return in last 50 episodes: -121.28\t Episode return: -132\t Total steps: 23298\n",
      "Avg return in last 50 episodes: -120.4\t Episode return: -90\t Total steps: 23834\n",
      "Avg return in last 50 episodes: -117.22\t Episode return: -90\t Total steps: 24355\n",
      "Avg return in last 50 episodes: -118.3\t Episode return: -151\t Total steps: 25047\n"
     ]
    }
   ],
   "source": [
    "// Training the agent for a total of at least 25000 steps.\n",
    "Train(25000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Total steps: 122\t Total reward: -121\n",
      "https://gym.kurg.org/b52506a7015d4/output.webm\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad6d8f816be24954b40b48957307872d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter widget"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.Deterministic() = true;\n",
    "\n",
    "// Creating and setting up the gym environment for testing.\n",
    "gym::Environment envTest(\"gym.kurg.org\", \"4040\", \"Acrobot-v1\");\n",
    "envTest.monitor.start(\"./dummy/\", true, true);\n",
    "\n",
    "// Resets the environment.\n",
    "envTest.reset();\n",
    "envTest.render();\n",
    "\n",
    "double totalReward = 0;\n",
    "size_t totalSteps = 0;\n",
    "\n",
    "// Testing the agent on gym's environment.\n",
    "while (1)\n",
    "{\n",
    "  // State from the environment is passed to the agent's internal representation.\n",
    "  agent.State().Data() = envTest.observation;\n",
    "\n",
    "  // With the given state, the agent selects an action according to its defined policy.\n",
    "  agent.SelectAction();\n",
    "\n",
    "  // Action to take, decided by the policy.\n",
    "  arma::mat action = {double(agent.Action().action)};\n",
    "\n",
    "  envTest.step(action);\n",
    "  totalReward += envTest.reward;\n",
    "  totalSteps += 1;\n",
    "\n",
    "  if (envTest.done)\n",
    "  {\n",
    "    std::cout << \" Total steps: \" << totalSteps << \"\\t Total reward: \"\n",
    "        << totalReward << std::endl;\n",
    "    break;\n",
    "  }\n",
    "\n",
    "  // Uncomment the following lines to see the reward and action in each step.\n",
    "  // std::cout << \" Current step: \" << totalSteps << \"\\t current reward: \"\n",
    "  //   << totalReward << \"\\t Action taken: \" << action;\n",
    "}\n",
    "\n",
    "envTest.close();\n",
    "std::string url = envTest.url();\n",
    "std::cout << url << std::endl;\n",
    "auto video = xw::video_from_url(url).finalize();\n",
    "video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the stochasticity of the environment, it's quite possible that sometimes the agent is not able to solve it in each test. So, we test the agent once more, just to be sure.\n",
    "\n",
    "You may test the agent any number of times by rerunning either of the testing cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Total steps: 151\t Total reward: -150\n",
      "https://gym.kurg.org/be13778abc714/output.webm\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74bd5f316a664f84bf16742466f2c6bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter widget"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.Deterministic() = true;\n",
    "\n",
    "// Creating and setting up the gym environment for testing.\n",
    "gym::Environment envTest(\"gym.kurg.org\", \"4040\", \"Acrobot-v1\");\n",
    "envTest.monitor.start(\"./dummy/\", true, true);\n",
    "\n",
    "// Resets the environment.\n",
    "envTest.reset();\n",
    "envTest.render();\n",
    "\n",
    "double totalReward = 0;\n",
    "size_t totalSteps = 0;\n",
    "\n",
    "// Testing the agent on gym's environment.\n",
    "while (1)\n",
    "{\n",
    "  // State from the environment is passed to the agent's internal representation.\n",
    "  agent.State().Data() = envTest.observation;\n",
    "\n",
    "  // With the given state, the agent selects an action according to its defined policy.\n",
    "  agent.SelectAction();\n",
    "\n",
    "  // Action to take, decided by the policy.\n",
    "  arma::mat action = {double(agent.Action().action)};\n",
    "\n",
    "  envTest.step(action);\n",
    "  totalReward += envTest.reward;\n",
    "  totalSteps += 1;\n",
    "\n",
    "  if (envTest.done)\n",
    "  {\n",
    "    std::cout << \" Total steps: \" << totalSteps << \"\\t Total reward: \"\n",
    "        << totalReward << std::endl;\n",
    "    break;\n",
    "  }\n",
    "\n",
    "  // Uncomment the following lines to see the reward and action in each step.\n",
    "  // std::cout << \" Current step: \" << totalSteps << \"\\t current reward: \"\n",
    "  //   << totalReward << \"\\t Action taken: \" << action;\n",
    "}\n",
    "\n",
    "envTest.close();\n",
    "std::string url = envTest.url();\n",
    "std::cout << url << std::endl;\n",
    "auto video = xw::video_from_url(url).finalize();\n",
    "video"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "C++14",
   "language": "C++14",
   "name": "xcpp14"
  },
  "language_info": {
   "codemirror_mode": "text/x-c++src",
   "file_extension": ".cpp",
   "mimetype": "text/x-c++src",
   "name": "c++",
   "version": "14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

/*!
@file tutorial.txt
@author Mehul Kumar Nirala
@brief Tutorial on Univariate Time Series using RNN.

@page rnntutorial LSTM Univariate Time Series

@section intro_lstmtut Introduction

An example of using Recurrent Neural Network (RNN) to make forcasts on a time series of international airline passengers, which we aim to solve using a simple LSTM neural network.

@section toc_lstmtut Table of Contents

This tutorial is split into the following sections:

 - \ref intro_lstmtut
 - \ref toc_lstmtut
 - \ref data_lstmtut
 - \ref model_lstmtut
 - \ref training_lstmtut
 - \ref results_lstmtut

@section data_lstmtut Time Series data
 We will look at the International Airline Passengers prediction problem.

 This is a problem where, given a year and a month, the task is to predict the number of international airline passengers in units of 1,000. The data ranges from January 1949 to December 1960, or 12 years, with 144 observations.

 
 Initially we normalize the input data using MinMaxScaler so that all the input features are on the scale from 0 to 1. Normally, it is a good idea to investigate various data preparation techniques to rescale the data and to make it stationary.

@code
  template<typename DataType = arma::mat>
  DataType MinMaxScaler(DataType& dataset)
  {
    arma::vec maxValues = arma::max(dataset, 1 /* for each dimension */);
    arma::vec minValues = arma::min(dataset, 1 /* for each dimension */);

    arma::vec rangeValues = maxValues - minValues;

    // Add a very small value if there are any zeros.
    rangeValues += 1e-25;

    dataset -= arma::repmat(minValues , 1, dataset.n_cols);
    dataset /= arma::repmat(rangeValues , 1, dataset.n_cols);
    return dataset;
  }
  ...
  // Scale data for increased numerical stability.
  dataset = MinMaxScaler(dataset);
@endcode


This will create a dataset where X is the number of passengers at a given time (t) and Y is the number of passengers at the next time (t + 1) over the period of 'rho' frames.

@code
  /*
   * The time series data for today should contain data for
   * past 'rho' days and the target variable.
   */
  template<typename InputDataType = arma::mat,
       typename DataType = arma::cube,
       typename LabelType = arma::cube>
  void CreateTimeSeriesData(InputDataType dataset, DataType& X, LabelType& y, size_t rho)
  {
    for(size_t i = 0;i<dataset.n_cols - rho - 2 ;i++)
    {
      X.subcube(span(), span(i), span()) = dataset.submat(span(), span(i, i+rho-1));
      y.subcube(span(), span(i), span()) = dataset.submat(span(), span(i+1, i+rho));
    }
  }
@endcode

@section model_lstmtut LSTM Model

We add 3 LSTM cells that will be stacked one after the other in the RNN, implementing an efficient stacked RNN. Finally, the output will have only one unit as this is a regression problem.

@code
  // No of timesteps to look in RNN.
  const int rho = 25;
  size_t inputSize = 5, outputSize = 2;

  // RNN model.
  model.Add<IdentityLayer<> >();
  model.Add<LSTM<> > (inputSize, outputSize, maxRho);
  model.Add<Dropout<> >(0.5);
  model.Add<LeakyReLU<> >();
  model.Add<LSTM<> > (outputSize, outputSize, maxRho);
  model.Add<Dropout<> >(0.5);
  model.Add<LeakyReLU<> >();
  model.Add<LSTM<> > (outputSize, outputSize, maxRho);
  model.Add<Linear<> >(outputSize, outputSize);

@endcode

Setting parameters Stochastic Gradient Descent (SGD) optimizer.
@code

  // Setting parameters Stochastic Gradient Descent (SGD) optimizer.
  SGD<AdamUpdate> optimizer(
    STEP_SIZE, // Step size of the optimizer.
    BATCH_SIZE, // Batch size. Number of data points that are used in each iteration.
    ITERATIONS_PER_EPOCH, // Max number of iterations.
    1e-8,// Tolerance.
    true,// Shuffle.
    AdamUpdate(1e-8, 0.9, 0.999)// Adam update policy.
  );

@endcode

@section training_lstmtut Training the model

@code
  cout << "Training ..." << endl;
  // Cycles for monitoring the process of a solution.
  for (int i = 0; i < EPOCH; i++)
  {
    // Train neural network. If this is the first iteration, weights are
    // random, using current values as starting point otherwise.
    model.Train(trainX, trainY, optimizer);

    // Don't reset optimizer's parameters between cycles.
    optimizer.ResetPolicy() = false;

    cube predOut;
    // Getting predictions on test data points.
    model.Predict(testX, predOut);

    // Calculating mse on test data points.
    double testMSE = MSE(predOut,testY);
    cout << i+1<< " - Mean Squared Error := "<< testMSE <<   endl;
  }
@endcode

@section results_lstmtut Results

Mean Squared error upto 10 iterations.
Training ...
1 - MeanSquaredError := 0.146075
2 - MeanSquaredError := 0.144882
3 - MeanSquaredError := 0.09501
4 - MeanSquaredError := 0.0875479
5 - MeanSquaredError := 0.0836975
6 - MeanSquaredError := 0.0796567
7 - MeanSquaredError := 0.0804368
8 - MeanSquaredError := 0.0803483
9 - MeanSquaredError := 0.0809061
10 - MeanSquaredError := 0.076797
....

*/